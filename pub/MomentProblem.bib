% Encoding: UTF-8

@PhdThesis{AuYeung2003,
  author   = {Susanna W. M. Au-Yeung},
  school   = {Imperial College of Science,Technology and Medicine (University of London), Department of Computing},
  title    = {Finding Probability Distributions From Moments},
  year     = {2003},
  address  = {London},
  month    = sep,
  type     = {mathesis},
  abstract = {Using the moment sequence of a continuous probability function to regenerate the full distribution is a mathematical problem that has been investigated for many years. One method is to use a flexible distribution to approximate the densities by matching the moments of the two distributions. The results are of great interest, as they can be readily applied to response time analysis in concurrent systems, where obtaining moments is relatively easy but obtaining full distributions is hard. In this dissertation we look at the development of a tool that approximates densities when given the first four moments. The Generalized Lambda Distribution will be discussed in detail and we shall investigate the effectiveness of using it to approximate densities of well known probability distributions in addition to densities derived from response time analysis models.},
  file     = {:D\:/Texte/02 Wissen/!Publikationen/2020 - MomentProblem/Au-Yeung (2003) Finding Probability Distributions From Moments.thesis.pdf:PDF},
}

@InProceedings{AuYeung2004,
  author    = {Au-Yeung, Susanna W. M. and Dingle, Nicholas J. and Knottenbelt, William J.},
  booktitle = {Proceedings of the 4th international workshop on Software and performance},
  title     = {Efficient approximation of response time densities and quantiles in stochastic models},
  year      = {2004},
  pages     = {151--155},
  abstract  = {Response time densities and quantiles are important performance and quality of service metrics, but their analytical derivation is, in general, very expensive. This paper presents a technique for determining approximate response time densities in Markov and semi-Markov stochastic models that requires two orders of magnitude less computation than exact Laplace transform-based techniques. The method computes the first four moments of the desired response time and then makes use of Generalised Lambda Distributions to obtain an approximation of the corresponding density. Numerical results show good agreement over a range of response time curves, particularly for those that are unimodal.},
}

@Misc{Weisstein2020,
  author       = {Eric Weisstein},
  howpublished = {MathWorld--A Wolfram Web Resource},
  month        = feb,
  note         = {last accessed 2020-03-15},
  title        = {Moment Problem},
  year         = {2020},
  readstatus   = {2020-03-15},
  url          = {http://mathworld.wolfram.com/MomentProblem.html},
}

@Article{Bishara2015,
  author       = {Bishara, Anthony J. and Hittner, James B.},
  journal      = {Educational and psychological measurement},
  title        = {Reducing Bias and Error in the Correlation Coefficient Due to Nonnormality.},
  year         = {2015},
  issn         = {1552-3888},
  month        = oct,
  pages        = {785--804},
  volume       = {75},
  abstract     = {It is more common for educational and psychological data to be nonnormal than to be approximately normal. This tendency may lead to bias and error in point estimates of the Pearson correlation coefficient. In a series of Monte Carlo simulations, the Pearson correlation was examined under conditions of normal and nonnormal data, and it was compared with its major alternatives, including the Spearman rank-order correlation, the bootstrap estimate, the Box-Cox transformation family, and a general normalizing transformation (i.e., rankit), as well as to various bias adjustments. Nonnormality caused the correlation coefficient to be inflated by up to +.14, particularly when the nonnormality involved heavy-tailed distributions. Traditional bias adjustments worsened this problem, further inflating the estimate. The Spearman and rankit correlations eliminated this inflation and provided conservative estimates. Rankit also minimized random error for most sample sizes, except for the smallest samples (  = 10), where bootstrapping was more effective. Overall, results justify the use of carefully chosen alternatives to the Pearson correlation when normality is violated.},
  country      = {United States},
  doi          = {10.1177/0013164414557639},
  groups       = {JS:6},
  issn-linking = {0013-1644},
  issue        = {5},
  keywords     = {Pearson; Spearman; correlation; nonnormal; normality; transformation},
  nlm-id       = {0372767},
  owner        = {NLM},
  pii          = {10.1177_0013164414557639},
  pmc          = {PMC5965513},
  pmid         = {29795841},
  pubmodel     = {Print-Electronic},
  pubstatus    = {ppublish},
  revised      = {2019-11-20},
}

@Article{Bishara2012,
  author    = {Anthony J. Bishara and James B. Hittner},
  journal   = {Psychological Methods},
  title     = {Testing the significance of a correlation with nonnormal data: Comparison of Pearson, Spearman, transformation, and resampling approaches.},
  year      = {2012},
  number    = {3},
  pages     = {399--417},
  volume    = {17},
  doi       = {10.1037/a0028087},
  publisher = {American Psychological Association ({APA})},
}

@TechReport{Hernandez2018,
  author   = {H. Hernandez},
  title    = {Comparison of Methods for the Reconstruction of Probability Density Functions from Data Samples},
  year     = {2018},
  number   = {FRR 2018-12},
  type     = {techreport},
  abstract = {Perhaps the best practice for predicting the outcome of any observable process is constructing robust mathematical models considering their most relevant factors. However, noise and randomness, caused by remaining factors not included in the model, will always be present. The behavior of random and randomistic variables (in a more general sense), can be mathematically described by the probability density function (PDF). It is therefore desirable to obtain PDFs for a measured variable, after a finite sample of data has been obtained. The identification of density functions fitting the data sample is denoted here as the reconstruction of the PDF. Such reconstruction is considered an inverse problem, since many different PDFs can satisfactorily describe the sample obtained. Furthermore, sampling always incorporates an inherent error in the process, given that the behavior of the sample may differ with respect to the behavior of the population, especially for small-sized samples. Thus, reconstructing PDFs is quite a challenging task. Different reconstruction methods, either based on the sample cumulative probability distribution or on the sample moments, are described and their performance is evaluated considering six different sets of data. Those test examples are samples obtained from populations with known probability distribution, allowing assessing the prediction capability of the reconstruction methods. If the type of distribution is known a priori, parametric reconstruction methods are found to be the best alternative. However, for unknown distributions, polynomial reconstruction methods provided good approximations for all cases considered. A selection of algorithms (in R language) used in the present work, is included in the Appendix.},
  doi      = {10.13140/RG.2.2.30177.35686},
  url      = {https://www.researchgate.net/profile/Hugo_Hernandez4/publication/329659261_Comparison_of_Methods_for_the_Reconstruction_of_Probability_Density_Functions_from_Data_Samples/links/5c13f92f92851c39ebedddbc/Comparison-of-Methods-for-the-Reconstruction-of-Probability-Density-Functions-from-Data-Samples.pdf},
}

@Comment{jabref-meta: databaseType:bibtex;}
